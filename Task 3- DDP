import torch
    import torch.distributed as dist
    from torch.nn.parallel import DistributedDataParallel

    def setup_ddp():
        # Initialize the process group
        dist.init_process_group(backend='nccl')

    def cleanup_ddp():
        # Clean up the process group
        dist.destroy_process_group()

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Set up DDP
    setup_ddp()

    # Define your model and loss function
    model = MyModel().to(device)
    model = DistributedDataParallel(model)

    criterion = torch.nn.MSELoss()

    # Define your optimizer
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    # Training loop
    for epoch in range(num_epochs):
        for batch in training_data:
            # Move batch to GPU
            inputs, labels = batch[0].to(device), batch[1].to(device)

            # Forward pass
            outputs = model(inputs)
            loss = criterion(outputs, labels)

            # Backward pass and optimization
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

    # Clean up DDP
    cleanup_ddp()
  
